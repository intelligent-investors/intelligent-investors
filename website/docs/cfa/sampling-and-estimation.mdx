# Sampling and Estimation

This topic review covers random samples and inferences about population means from sample data. It is essential that you know the central limit theorem, for it allows us to use sampling statistics to construct confidence intervals for point estimates of population means. Make sure you can calculate confidence intervals for population means given sample parameter estimates and a level of significance, and know when it is appropriate to use the $z$-statistic versus the $t$-statistic. You should also understand the basic procedures for creating random samples, and recognize the warning signs of various sampling biases from nonrandom samples.

## 1: CENTRAL LIMIT THEORM AND STANDARD ERROR

In many real-world statistics applications, it is impractical (or impossible) to study an entire population. When this is the case, a subgroup of the population, called a sample, can be evaluated. Based upon this sample, the parameters of the underlying population can be estimated.

For example, rather than attempting to measure the performance of the U.S. stock market by observing the performance of all 10,000 or so stocks trading in the United States at any one time, the performance of the subgroup of 500 stocks in the S&P 500 can be measured. The results of the statistical analysis of this sample can then be used to draw conclusions about the entire population of U.S. stocks.

### A: Define simple random sampling and a sampling distribution.

### B: Explain sampling error.

**Simple random sampling** is a method of selecting a sample in such a way that each item or person in the population being studied has the same likelihood of being included in the sample. As an example of simple random sampling, assume that you want to draw a sample of five items out of a group of 50 items. This can be accomplished by numbering each of the 50 items, placing them in a hat, and shaking the hat. Next, one number can be drawn randomly from the hat. Repeating this process (experiment) four more times results in a set of five numbers. The five drawn numbers (items) comprise a simple random sample from the population. In applications like this one, a random-number table or a computer random number generator is often used to create the sample. Another way to form an approximately random sample is **systematic sampling**, selecting every nth member from a population. **Sampling error** is the difference between a sample statistic (the mean, variance, or standard deviation of the sample) and its corresponding population parameter (the true mean, variance, or standard deviation of the population). For example, the sampling error for the mean is as follows:

$$
\text{sampling error of the mean} = \text{sample mean} - \text{population mean} = \bar{x} - \mu
$$

#### A Sampling Distribution

It is important to recognize that the sample statistic itself is a random variable and, therefore, has a probability distribution. The **sampling distribution** of the sample statistic is a probability distribution of all possible sample statistics computed from a set of equal-size samples that were randomly drawn from the same population. Think of it as the probability distribution of a statistic from many samples.

For example, suppose a random sample of 100 bonds is selected from a population of a major municipal bond index consisting of 1,000 bonds, and then the mean return of the 100-bond sample is calculated. Repeating this process many times will result in many different estimates of the population mean return (i.e., one for each sample). The distribution of these estimates of the mean is the **sampling distribution of the mean**.

It is important to note that this sampling distribution is distinct from the distribution of the actual prices of the 1,000 bonds in the underlying population and will have different parameters.

### C: Distinguish between simple random and stratified random sampling.

**Stratified random sampling** uses a classification system to separate the population into smaller groups based on one or more distinguishing characteristics. From each subgroup, or stratum, a random sample is taken and the results are pooled. The size of the samples from each stratum is based on the size of the stratum relative to the population.

Stratified sampling is often used in bond indexing because of the difficulty and cost of completely replicating the entire population of bonds. In this case, bonds in a population are categorized (stratified) according to major bond risk factors such as duration, maturity, coupon rate, and the like. Then, samples are drawn from each separate category and combined to form a final sample.

To see how this works, suppose you want to construct a bond portfolio that is indexed to the major municipal bond index using a stratified random sampling approach. First, the entire population of 1,000 municipal bonds in the index can be classified on the basis of maturity and coupon rate. Then, cells (stratum) can be created for different maturity/coupon combinations, and random samples can be drawn from each of the maturity/coupon cells. To sample from a cell containing 50 bonds with 2- to 4-year maturities and coupon rates less than 5%, we would select five bonds. The number of bonds drawn from a given cell corresponds to the cell’s weight relative to the population (index), or (50 / 1000) x (100) = 5 bonds. This process is repeated for all of the maturity/coupon cells, and the individual samples are combined to form the portfolio.

By using stratified sampling, we guarantee that we sample five bonds from this cell. If we had used simple random sampling, there would be no guarantee that we would sample any of the bonds in the cell. Or, we may have selected more than five bonds from this cell.

### D: Distinguish between time-series and cross-sectional data.

**Time-series data** consist of observations taken *over a period of time* at specific and equally spaced time intervals. The set of monthly returns on Microsoft stock from January 1994 to January 2004 is an example of a time-series data sample.

**Cross-sectional data** are a sample of observations taken *at a single point in time*. The sample of reported earnings per share of all Nasdaq companies as of December 31, 2004, is an example of a cross-sectional data sample.

Time-series and cross-sectional data can be pooled in the same data set. **Longitudinal data** are observations over time of multiple characteristics of the same entity, such as unemployment, inflation, and GDP growth rates for a country over 10 years. **Panel data** contain observations over time of the same characteristic for multiple entities, such as debt/equity ratios for 20 companies over the most recent 24 quarters. Panel and longitudinal data are typically presented in table or spreadsheet form.

### E: Explain the central limit theorem and its importance.

The **central limit theorem** states that for simple random samples of size n from a population with a mean $μ$ and a finite variance $$\sigma^2$$, the sampling distribution of the sample mean $$\bar{x}$$ approaches a normal probability distribution with mean μ and a variance equal to $$\frac{\sigma^2}{n}$$ as the sample size becomes large.

The central limit theorem is extremely useful because the normal distribution is relatively easy to apply to hypothesis testing and to the construction of confidence intervals. Specific inferences about the population mean can be made from the sample mean, **regardless of the population’s distribution**, as long as the sample size is “sufficiently large,” which usually means n ≥ 30.

**Important properties of the central limit theorem include the following:**

- If the sample size n is sufficiently large (n ≥ 30), the sampling distribution of the sample means will be approximately normal. Remember what’s going on here, random samples of size n are repeatedly being taken from an overall larger population. Each of these random samples has its own mean, which is itself a random variable, and this set of sample means has a distribution that is approximately normal.
- The mean of the population, μ, and the mean of the distribution of all possible sample means are equal.
- The variance of the distribution of sample means is $$\frac{\sigma^2}{n}$$, the population variance divided by the sample size.

### F: Calculate and interpret the standard error of the sample mean.

The **standard error of the sample mean** is the standard deviation of the distribution of the sample means.

When the standard deviation of the population, $\sigma$, is known, the standard error of the sample mean is calculated as:

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
$$

where:

* $\sigma_{\bar{x}}$ = standard error of the sample mean  
* $\sigma$ = standard deviation of the population  
* $n$ = size of the sample

:::info[**EXAMPLE**: Standard error of sample mean (known population variance)]
The mean hourly wage for Iowa farm workers is \$13.50 with a population standard deviation of \$2.90. Calculate and interpret the standard error of the sample mean for a sample size of 30.

**Answer:**

Because the population standard deviation, $\sigma$, is known, the standard error of the sample mean is expressed as:

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{2.90}{\sqrt{30}} = \$0.53
$$

This means that if we were to take many samples of size 30 from the Iowa farm worker population and prepare a sampling distribution of the sample means, we would get a distribution with an expected mean of \$13.50 and standard error (standard deviation of the sample means) of \$0.53.

Practically speaking, the population’s standard deviation is almost never known. Instead, the standard error of the sample mean must be estimated by dividing the standard deviation of the sample by $\sqrt{n}$:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}}
$$
:::

:::info[**EXAMPLE**: Standard error of sample mean (unknown population variance)]
Suppose a sample contains the past 30 monthly returns for McCreary, Inc. The mean return is 2% and the sample standard deviation is 20%. Calculate and interpret the standard error of the sample mean.

**Answer:**

Since $\sigma$ is unknown, the standard error of the sample mean is:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{20\%}{\sqrt{30}} = 3.6\%
$$

This implies that if we took all possible samples of size 30 from McCreary’s monthly returns and prepared a sampling distribution of the sample means, the mean would be 2% with a standard error of 3.6%.
:::

:::info[**EXAMPLE**: Standard error of sample mean (unknown population variance)]
Continuing with our example, suppose that instead of a sample size of 30, we take a sample of the past 200 monthly returns for McCreary, Inc. In order to highlight the effect of sample size on the sample standard error, let’s assume that the mean return and standard deviation of this larger sample remain at 2% and 20%, respectively. Now, calculate the standard error of the sample mean for the 200-return sample.

**Answer:**

The standard error of the sample mean is computed as:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{20\%}{\sqrt{200}} = 1.4\%
$$
:::

The result of the preceding two examples illustrates an important property of sampling distributions. Notice that the value of the standard error of the sample mean decreased from 3.6% to 1.4% as the sample size increased from 30 to 200. This is because as the sample size increases, the sample mean gets closer, on average, to the true mean of the population. In other words, the distribution of the sample means about the population mean gets smaller and smaller, so the standard error of the sample mean decreases.

:::note[**PROFESSOR'S NOTE**]
I get a lot of questions about when to use $$\sigma$$ and $$\sigma / \sqrt{n}$$. Just remember that the standard deviation of the means of multiple samples is less than the standard deviation of single observations. If the standard deviation of monthly stock returns is 2%, the standard error (deviation) of the average monthly return over the next six months is $$2\% / \sqrt{6} = 0.82\%$$. The average of several observations of a random variable will be less widely dispersed (have lower standard deviation) around the expected value than will a single observation of the random variable.
:::

### G: Identify and describe desirable properties of an estimator.

Regardless of whether we are concerned with point estimates or confidence intervals, there are certain statistical properties that make some estimators more desirable than others. These desirable properties of an estimator are unbiasedness, efficiency, and consistency.

- An **unbiased** estimator is one for which the expected value of the estimator is equal to the parameter you are trying to estimate. For example, because the expected value of the sample mean is equal to the population mean $$\mathbb{E}(\bar{x}) = \mu$$, the sample mean is an unbiased estimator of the population mean.
- An unbiased estimator is also **efficient** if the variance of its sampling distribution is smaller than all the other unbiased estimators of the parameter you are trying to estimate. The sample mean, for example, is an unbiased and efficient estimator of the population mean.
- A **consistent** estimator is one for which the accuracy of the parameter estimate increases as the sample size increases. As the sample size increases, the standard error of the sample mean falls, and the sampling distribution bunches more closely around the population mean. In fact, as the sample size approaches infinity, the standard error approaches zero.

### 📝 QUIZ

1. A simple random sample is a sample drawn in such a way that each member of the population has:

* A. some chance of being selected in the sample.
* B. an equal chance of being included in the sample.
* C. a 1% chance of being included in the sample.

2. The sampling distribution of a statistic is the probability distribution made up of all possible:

* A. observations from the underlying population.
* B. sample statistics computed from samples of varying sizes drawn from the same population.
* C. sample statistics computed from samples of the same size drawn from the same population.

3. Sampling error is defined as:

* A. an error that occurs when a sample of less than 30 elements is drawn.
* B. an error that occurs during collection, recording, and tabulation of data.
* C. the difference between the value of a sample statistic and the value of the corresponding population parameter.

4. The mean age of all CFA candidates is 28 years. The mean age of a random sample of 100 candidates is found to be 26.5 years. The difference of 1.5 years is called:

* A. the random error.
* B. the sampling error.
* C. the population error.

5. The sample of debt/equity ratios of 25 publicly traded U.S. banks as of fiscal year-end 2003 is an example of:

* A. a point estimate.
* B. cross-sectional data.
* C. a stratified random sample.

6. To apply the central limit theorem to the sampling distribution of the sample mean, the sample is usually considered to be large if $$n$$ is greater than:

* A. 20.
* B. 25.
* C. 30.

7. If $$n$$ is large and the population standard deviation is unknown, the standard error of the sampling distribution of the sample mean is equal to:

* A. the sample standard deviation divided by the sample size.
* B. the population standard deviation multiplied by the sample size.
* C. the sample standard deviation divided by the square root of the sample size.

8. The standard error of the sampling distribution of the sample mean for a sample size of $$n$$ drawn from a population with a mean of $$\mu$$ and a standard deviation of $$\sigma$$ is:

* A. sample standard deviation divided by the sample size.
* B. sample standard deviation divided by the square root of the sample size.
* C. population standard deviation divided by the square root of the sample size.

9. Assume that a population has a mean of 14 with a standard deviation of 2. If a random sample of 49 observations is drawn from this population, the standard error of the sample mean is closest to:

* A. 0.04.
* B. 0.29.
* C. 2.00.

10. The population’s mean is 30 and the mean of a sample of size 100 is 28.5. The variance of the sample is 25. The standard error of the sample mean is closest to:

* A. 0.05.
* B. 0.25.
* C. 0.50.

11. Which of the following is least likely a desirable property of an estimator?

* A. Reliability.
* B. Efficiency.
* C. Consistency.

## 2: CONFIDENCE INTERVALS AND t-DISTRIBUTION

### H: Distinguish between a point estimate and a confidence interval estimate of a population parameter.

**Point estimates** are single (sample) values used to estimate population parameters. The formula used to compute the point estimate is called the estimator. For example, the sample mean, $$ \bar{x} $$, is an estimator of the population mean $$ \mu $$ and is computed using the familiar formula:

$$
\bar{x} = \frac{\sum x}{n}
$$

The value generated with this calculation for a given sample is called the **point estimate** of the mean.

A **confidence interval** is a range of values in which the population parameter is expected to lie. The construction of confidence intervals is described later in this topic review.

### I: Describe properties of Student’s t-distribution and calculate and interpret its degrees of freedom.

**Student’s t-distribution**, or simply the **t-distribution**, is a bell-shaped probability distribution that is symmetrical about its mean. It is the appropriate distribution to use when constructing confidence intervals based on **small samples** (n < 30) from populations with **unknown variance** and a normal, or approximately normal, distribution. It may also be appropriate to use the **t-distribution** when the population variance is unknown and the sample size is large enough that the central limit theorem will assure that the sampling distribution is approximately normal.

**Student’s t-distribution has the following properties:**

- It is symmetrical.
- It is defined by a single parameter, the **degrees of freedom** ($df$), where the degrees of freedom are equal to the number of sample observations minus 1, $n – 1$, for sample means.
- It has more probability in the tails (“fatter tails”) than the normal distribution.
- As the degrees of freedom (the sample size) gets larger, the shape of the t-distribution more closely approaches a standard normal distribution.

When **compared to the normal distribution**, the t-distribution is flatter with more area under the tails (i.e., it has fatter tails). As the degrees of freedom for the t-distribution increase, however, its shape approaches that of the normal distribution.

The degrees of freedom for tests based on sample means are $n – 1$ because, given the mean, only $n – 1$ observations can be unique.

The **t-distribution** is a symmetrical distribution that is centered about zero. The shape of the t-distribution is dependent on the number of degrees of freedom, and degrees of freedom are based on the number of sample observations. The t-distribution is flatter and has thicker tails than the standard normal distribution. As the number of observations increases (i.e., the degrees of freedom increase), the $t$-distribution becomes more spiked and its tails become thinner. As the number of degrees of freedom increases without bound, the t-distribution converges to the standard normal distribution ($z$-distribution). The thickness of the tails relative to those of the $z$-distribution is important in hypothesis testing because thicker tails mean more observations away from the center of the distribution (more outliers). Hence, hypothesis testing using the $t$-distribution makes it more difficult to reject the null relative to hypothesis testing using the $z$-distribution.

The following table contains one-tailed critical values for the $t$-distribution at the 0.05 and 0.025 levels of significance with various degrees of freedom ($df$). Note that, unlike the $z$-table, the $t$-values are contained within the table, and the probabilities are located at the column headings. Also note that the level of significance of a $t$-test corresponds to the *one-tailed probabilities, p*, that head the columns in the t-table.

**Table of Critical t-Values**

| **One-Tailed Probabilities, p** |||
|-----------------------------|---------|---------|
| df                          | p = 0.05| p = 0.025|
| 5                           | 2.015   | 2.571   |
| 10                          | 1.812   | 2.228   |
| 15                          | 1.753   | 2.131   |
| 20                          | 1.725   | 2.086   |
| 25                          | 1.708   | 2.060   |
| 30                          | 1.697   | 2.042   |
| 40                          | 1.684   | 2.021   |
| 50                          | 1.676   | 2.009   |
| 60                          | 1.671   | 2.000   |
| 70                          | 1.667   | 1.994   |
| 80                          | 1.664   | 1.990   |
| 90                          | 1.662   | 1.987   |
| 100                         | 1.660   | 1.984   |
| 120                         | 1.658   | 1.980   |
| ∞                           | 1.645   | 1.960   |

The following figure illustrates the different shapes of the $t$-distribution associated with different degrees of freedom. The tendency is for the $t$-distribution to look more and more like the normal distribution as the degrees of freedom increase. Practically speaking, the greater the degrees of freedom, the greater the percentage of observations near the center of the distribution and the lower the percentage of observations in the tails, which are thinner as degrees of freedom increase. This means that confidence intervals for a random variable that follows a $t$-distribution must be wider (narrower) when the degrees of freedom are less (more) for a given significance level.

**Figure: $t$-Distributions for Different Degrees of Freedom ($\text{df}$)**
