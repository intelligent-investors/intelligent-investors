import FigureA from "./images/sampling-and-estimation/figureA.png";

# Sampling and Estimation

This topic review covers random samples and inferences about population means from sample data. It is essential that you know the central limit theorem, for it allows us to use sampling statistics to construct confidence intervals for point estimates of population means. Make sure you can calculate confidence intervals for population means given sample parameter estimates and a level of significance, and know when it is appropriate to use the $z$-statistic versus the $t$-statistic. You should also understand the basic procedures for creating random samples, and recognize the warning signs of various sampling biases from nonrandom samples.

## 1: CENTRAL LIMIT THEORM AND STANDARD ERROR

In many real-world statistics applications, it is impractical (or impossible) to study an entire population. When this is the case, a subgroup of the population, called a sample, can be evaluated. Based upon this sample, the parameters of the underlying population can be estimated.

For example, rather than attempting to measure the performance of the U.S. stock market by observing the performance of all 10,000 or so stocks trading in the United States at any one time, the performance of the subgroup of 500 stocks in the S&P 500 can be measured. The results of the statistical analysis of this sample can then be used to draw conclusions about the entire population of U.S. stocks.

### A: Define simple random sampling and a sampling distribution.

### B: Explain sampling error.

**Simple random sampling** is a method of selecting a sample in such a way that each item or person in the population being studied has the same likelihood of being included in the sample. As an example of simple random sampling, assume that you want to draw a sample of five items out of a group of 50 items. This can be accomplished by numbering each of the 50 items, placing them in a hat, and shaking the hat. Next, one number can be drawn randomly from the hat. Repeating this process (experiment) four more times results in a set of five numbers. The five drawn numbers (items) comprise a simple random sample from the population. In applications like this one, a random-number table or a computer random number generator is often used to create the sample. Another way to form an approximately random sample is **systematic sampling**, selecting every nth member from a population. **Sampling error** is the difference between a sample statistic (the mean, variance, or standard deviation of the sample) and its corresponding population parameter (the true mean, variance, or standard deviation of the population). For example, the sampling error for the mean is as follows:

$$
\text{sampling error of the mean} = \text{sample mean} - \text{population mean} = \bar{x} - \mu
$$

#### A Sampling Distribution

It is important to recognize that the sample statistic itself is a random variable and, therefore, has a probability distribution. The **sampling distribution** of the sample statistic is a probability distribution of all possible sample statistics computed from a set of equal-size samples that were randomly drawn from the same population. Think of it as the probability distribution of a statistic from many samples.

For example, suppose a random sample of 100 bonds is selected from a population of a major municipal bond index consisting of 1,000 bonds, and then the mean return of the 100-bond sample is calculated. Repeating this process many times will result in many different estimates of the population mean return (i.e., one for each sample). The distribution of these estimates of the mean is the **sampling distribution of the mean**.

It is important to note that this sampling distribution is distinct from the distribution of the actual prices of the 1,000 bonds in the underlying population and will have different parameters.

### C: Distinguish between simple random and stratified random sampling.

**Stratified random sampling** uses a classification system to separate the population into smaller groups based on one or more distinguishing characteristics. From each subgroup, or stratum, a random sample is taken and the results are pooled. The size of the samples from each stratum is based on the size of the stratum relative to the population.

Stratified sampling is often used in bond indexing because of the difficulty and cost of completely replicating the entire population of bonds. In this case, bonds in a population are categorized (stratified) according to major bond risk factors such as duration, maturity, coupon rate, and the like. Then, samples are drawn from each separate category and combined to form a final sample.

To see how this works, suppose you want to construct a bond portfolio that is indexed to the major municipal bond index using a stratified random sampling approach. First, the entire population of 1,000 municipal bonds in the index can be classified on the basis of maturity and coupon rate. Then, cells (stratum) can be created for different maturity/coupon combinations, and random samples can be drawn from each of the maturity/coupon cells. To sample from a cell containing 50 bonds with 2- to 4-year maturities and coupon rates less than 5%, we would select five bonds. The number of bonds drawn from a given cell corresponds to the cell’s weight relative to the population (index), or (50 / 1000) x (100) = 5 bonds. This process is repeated for all of the maturity/coupon cells, and the individual samples are combined to form the portfolio.

By using stratified sampling, we guarantee that we sample five bonds from this cell. If we had used simple random sampling, there would be no guarantee that we would sample any of the bonds in the cell. Or, we may have selected more than five bonds from this cell.

### D: Distinguish between time-series and cross-sectional data.

**Time-series data** consist of observations taken *over a period of time* at specific and equally spaced time intervals. The set of monthly returns on Microsoft stock from January 1994 to January 2004 is an example of a time-series data sample.

**Cross-sectional data** are a sample of observations taken *at a single point in time*. The sample of reported earnings per share of all Nasdaq companies as of December 31, 2004, is an example of a cross-sectional data sample.

Time-series and cross-sectional data can be pooled in the same data set. **Longitudinal data** are observations over time of multiple characteristics of the same entity, such as unemployment, inflation, and GDP growth rates for a country over 10 years. **Panel data** contain observations over time of the same characteristic for multiple entities, such as debt/equity ratios for 20 companies over the most recent 24 quarters. Panel and longitudinal data are typically presented in table or spreadsheet form.

### E: Explain the central limit theorem and its importance.

The **central limit theorem** states that for simple random samples of size n from a population with a mean $μ$ and a finite variance $$\sigma^2$$, the sampling distribution of the sample mean $$\bar{x}$$ approaches a normal probability distribution with mean μ and a variance equal to $$\frac{\sigma^2}{n}$$ as the sample size becomes large.

The central limit theorem is extremely useful because the normal distribution is relatively easy to apply to hypothesis testing and to the construction of confidence intervals. Specific inferences about the population mean can be made from the sample mean, **regardless of the population’s distribution**, as long as the sample size is “sufficiently large,” which usually means n ≥ 30.

**Important properties of the central limit theorem include the following:**

- If the sample size n is sufficiently large (n ≥ 30), the sampling distribution of the sample means will be approximately normal. Remember what’s going on here, random samples of size n are repeatedly being taken from an overall larger population. Each of these random samples has its own mean, which is itself a random variable, and this set of sample means has a distribution that is approximately normal.
- The mean of the population, μ, and the mean of the distribution of all possible sample means are equal.
- The variance of the distribution of sample means is $$\frac{\sigma^2}{n}$$, the population variance divided by the sample size.

### F: Calculate and interpret the standard error of the sample mean.

The **standard error of the sample mean** is the standard deviation of the distribution of the sample means.

When the standard deviation of the population, $\sigma$, is known, the standard error of the sample mean is calculated as:

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
$$

where:

* $\sigma_{\bar{x}}$ = standard error of the sample mean  
* $\sigma$ = standard deviation of the population  
* $n$ = size of the sample

:::info[**EXAMPLE**: Standard error of sample mean (known population variance)]
The mean hourly wage for Iowa farm workers is \$13.50 with a population standard deviation of \$2.90. Calculate and interpret the standard error of the sample mean for a sample size of 30.

**Answer:**

Because the population standard deviation, $\sigma$, is known, the standard error of the sample mean is expressed as:

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{2.90}{\sqrt{30}} = \$0.53
$$

This means that if we were to take many samples of size 30 from the Iowa farm worker population and prepare a sampling distribution of the sample means, we would get a distribution with an expected mean of \$13.50 and standard error (standard deviation of the sample means) of \$0.53.

Practically speaking, the population’s standard deviation is almost never known. Instead, the standard error of the sample mean must be estimated by dividing the standard deviation of the sample by $\sqrt{n}$:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}}
$$
:::

:::info[**EXAMPLE**: Standard error of sample mean (unknown population variance)]
Suppose a sample contains the past 30 monthly returns for McCreary, Inc. The mean return is 2% and the sample standard deviation is 20%. Calculate and interpret the standard error of the sample mean.

**Answer:**

Since $\sigma$ is unknown, the standard error of the sample mean is:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{20\%}{\sqrt{30}} = 3.6\%
$$

This implies that if we took all possible samples of size 30 from McCreary’s monthly returns and prepared a sampling distribution of the sample means, the mean would be 2% with a standard error of 3.6%.
:::

:::info[**EXAMPLE**: Standard error of sample mean (unknown population variance)]
Continuing with our example, suppose that instead of a sample size of 30, we take a sample of the past 200 monthly returns for McCreary, Inc. In order to highlight the effect of sample size on the sample standard error, let’s assume that the mean return and standard deviation of this larger sample remain at 2% and 20%, respectively. Now, calculate the standard error of the sample mean for the 200-return sample.

**Answer:**

The standard error of the sample mean is computed as:

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{20\%}{\sqrt{200}} = 1.4\%
$$
:::

The result of the preceding two examples illustrates an important property of sampling distributions. Notice that the value of the standard error of the sample mean decreased from 3.6% to 1.4% as the sample size increased from 30 to 200. This is because as the sample size increases, the sample mean gets closer, on average, to the true mean of the population. In other words, the distribution of the sample means about the population mean gets smaller and smaller, so the standard error of the sample mean decreases.

:::note[**PROFESSOR'S NOTE**]
I get a lot of questions about when to use $$\sigma$$ and $$\sigma / \sqrt{n}$$. Just remember that the standard deviation of the means of multiple samples is less than the standard deviation of single observations. If the standard deviation of monthly stock returns is 2%, the standard error (deviation) of the average monthly return over the next six months is $$2\% / \sqrt{6} = 0.82\%$$. The average of several observations of a random variable will be less widely dispersed (have lower standard deviation) around the expected value than will a single observation of the random variable.
:::

### G: Identify and describe desirable properties of an estimator.

Regardless of whether we are concerned with point estimates or confidence intervals, there are certain statistical properties that make some estimators more desirable than others. These desirable properties of an estimator are unbiasedness, efficiency, and consistency.

- An **unbiased** estimator is one for which the expected value of the estimator is equal to the parameter you are trying to estimate. For example, because the expected value of the sample mean is equal to the population mean $$\mathbb{E}(\bar{x}) = \mu$$, the sample mean is an unbiased estimator of the population mean.
- An unbiased estimator is also **efficient** if the variance of its sampling distribution is smaller than all the other unbiased estimators of the parameter you are trying to estimate. The sample mean, for example, is an unbiased and efficient estimator of the population mean.
- A **consistent** estimator is one for which the accuracy of the parameter estimate increases as the sample size increases. As the sample size increases, the standard error of the sample mean falls, and the sampling distribution bunches more closely around the population mean. In fact, as the sample size approaches infinity, the standard error approaches zero.

### 📝 QUIZ

1. A simple random sample is a sample drawn in such a way that each member of the population has:

* A. some chance of being selected in the sample.
* B. an equal chance of being included in the sample.
* C. a 1% chance of being included in the sample.

2. The sampling distribution of a statistic is the probability distribution made up of all possible:

* A. observations from the underlying population.
* B. sample statistics computed from samples of varying sizes drawn from the same population.
* C. sample statistics computed from samples of the same size drawn from the same population.

3. Sampling error is defined as:

* A. an error that occurs when a sample of less than 30 elements is drawn.
* B. an error that occurs during collection, recording, and tabulation of data.
* C. the difference between the value of a sample statistic and the value of the corresponding population parameter.

4. The mean age of all CFA candidates is 28 years. The mean age of a random sample of 100 candidates is found to be 26.5 years. The difference of 1.5 years is called:

* A. the random error.
* B. the sampling error.
* C. the population error.

5. The sample of debt/equity ratios of 25 publicly traded U.S. banks as of fiscal year-end 2003 is an example of:

* A. a point estimate.
* B. cross-sectional data.
* C. a stratified random sample.

6. To apply the central limit theorem to the sampling distribution of the sample mean, the sample is usually considered to be large if $$n$$ is greater than:

* A. 20.
* B. 25.
* C. 30.

7. If $$n$$ is large and the population standard deviation is unknown, the standard error of the sampling distribution of the sample mean is equal to:

* A. the sample standard deviation divided by the sample size.
* B. the population standard deviation multiplied by the sample size.
* C. the sample standard deviation divided by the square root of the sample size.

8. The standard error of the sampling distribution of the sample mean for a sample size of $$n$$ drawn from a population with a mean of $$\mu$$ and a standard deviation of $$\sigma$$ is:

* A. sample standard deviation divided by the sample size.
* B. sample standard deviation divided by the square root of the sample size.
* C. population standard deviation divided by the square root of the sample size.

9. Assume that a population has a mean of 14 with a standard deviation of 2. If a random sample of 49 observations is drawn from this population, the standard error of the sample mean is closest to:

* A. 0.04.
* B. 0.29.
* C. 2.00.

10. The population’s mean is 30 and the mean of a sample of size 100 is 28.5. The variance of the sample is 25. The standard error of the sample mean is closest to:

* A. 0.05.
* B. 0.25.
* C. 0.50.

11. Which of the following is least likely a desirable property of an estimator?

* A. Reliability.
* B. Efficiency.
* C. Consistency.

## 2: CONFIDENCE INTERVALS AND t-DISTRIBUTION

### H: Distinguish between a point estimate and a confidence interval estimate of a population parameter.

**Point estimates** are single (sample) values used to estimate population parameters. The formula used to compute the point estimate is called the estimator. For example, the sample mean, $$ \bar{x} $$, is an estimator of the population mean $$ \mu $$ and is computed using the familiar formula:

$$
\bar{x} = \frac{\sum x}{n}
$$

The value generated with this calculation for a given sample is called the **point estimate** of the mean.

A **confidence interval** is a range of values in which the population parameter is expected to lie. The construction of confidence intervals is described later in this topic review.

### I: Describe properties of Student’s t-distribution and calculate and interpret its degrees of freedom.

**Student’s t-distribution**, or simply the **t-distribution**, is a bell-shaped probability distribution that is symmetrical about its mean. It is the appropriate distribution to use when constructing confidence intervals based on **small samples** (n < 30) from populations with **unknown variance** and a normal, or approximately normal, distribution. It may also be appropriate to use the **t-distribution** when the population variance is unknown and the sample size is large enough that the central limit theorem will assure that the sampling distribution is approximately normal.

**Student’s t-distribution has the following properties:**

- It is symmetrical.
- It is defined by a single parameter, the **degrees of freedom** ($df$), where the degrees of freedom are equal to the number of sample observations minus 1, $n – 1$, for sample means.
- It has more probability in the tails (“fatter tails”) than the normal distribution.
- As the degrees of freedom (the sample size) gets larger, the shape of the t-distribution more closely approaches a standard normal distribution.

When **compared to the normal distribution**, the t-distribution is flatter with more area under the tails (i.e., it has fatter tails). As the degrees of freedom for the t-distribution increase, however, its shape approaches that of the normal distribution.

The degrees of freedom for tests based on sample means are $n – 1$ because, given the mean, only $n – 1$ observations can be unique.

The **t-distribution** is a symmetrical distribution that is centered about zero. The shape of the t-distribution is dependent on the number of degrees of freedom, and degrees of freedom are based on the number of sample observations. The t-distribution is flatter and has thicker tails than the standard normal distribution. As the number of observations increases (i.e., the degrees of freedom increase), the $t$-distribution becomes more spiked and its tails become thinner. As the number of degrees of freedom increases without bound, the t-distribution converges to the standard normal distribution ($z$-distribution). The thickness of the tails relative to those of the $z$-distribution is important in hypothesis testing because thicker tails mean more observations away from the center of the distribution (more outliers). Hence, hypothesis testing using the $t$-distribution makes it more difficult to reject the null relative to hypothesis testing using the $z$-distribution.

The following table contains one-tailed critical values for the $t$-distribution at the 0.05 and 0.025 levels of significance with various degrees of freedom ($df$). Note that, unlike the $z$-table, the $t$-values are contained within the table, and the probabilities are located at the column headings. Also note that the level of significance of a $t$-test corresponds to the *one-tailed probabilities, p*, that head the columns in the t-table.

**Table of Critical t-Values**

| **One-Tailed Probabilities, p** |||
|-----------------------------|---------|---------|
| df                          | p = 0.05| p = 0.025|
| 5                           | 2.015   | 2.571   |
| 10                          | 1.812   | 2.228   |
| 15                          | 1.753   | 2.131   |
| 20                          | 1.725   | 2.086   |
| 25                          | 1.708   | 2.060   |
| 30                          | 1.697   | 2.042   |
| 40                          | 1.684   | 2.021   |
| 50                          | 1.676   | 2.009   |
| 60                          | 1.671   | 2.000   |
| 70                          | 1.667   | 1.994   |
| 80                          | 1.664   | 1.990   |
| 90                          | 1.662   | 1.987   |
| 100                         | 1.660   | 1.984   |
| 120                         | 1.658   | 1.980   |
| ∞                           | 1.645   | 1.960   |

The following figure illustrates the different shapes of the $t$-distribution associated with different degrees of freedom. The tendency is for the $t$-distribution to look more and more like the normal distribution as the degrees of freedom increase. Practically speaking, the greater the degrees of freedom, the greater the percentage of observations near the center of the distribution and the lower the percentage of observations in the tails, which are thinner as degrees of freedom increase. This means that confidence intervals for a random variable that follows a $t$-distribution must be wider (narrower) when the degrees of freedom are less (more) for a given significance level.

**Figure: $t$-Distributions for Different Degrees of Freedom ($\text{df}$)**

<div style={{ textAlign: 'center' }}>
<img src={FigureA} style={{width: 800}} />
</div>

### J: Calculate and interpret a confidence interval for a population mean, given a normal distribution with 1) a known population variance, 2) an unknown population variance, or 3) an unknown population variance and a large sample size.

Confidence interval estimates result in a range of values within which the actual value of a parameter will lie, given the probability of $$1 - \alpha$$. Here, alpha, $$\alpha$$, is called the level of significance for the confidence interval, and the probability $$1 - \alpha$$ is referred to as the degree of confidence. For example, we might estimate that the population mean of random variables will range from 15 to 25 with a 95$$\%$$ degree of confidence, or at the 5$$\%$$ level of significance.

Confidence intervals are usually constructed by adding or subtracting an appropriate value from the point estimate. In general, confidence intervals take on the following form:

$$
\text{point estimate} \pm (\text{reliability factor} \times \text{standard error})
$$

where:

- point estimate = value of a sample statistic of the population parameter
- reliability factor = number that depends on the sampling distribution of the point estimate and the probability that the point estimate falls in the confidence interval, $$1 - \alpha$$
- standard error = standard error of the point estimate

If the population has a normal distribution with a known variance, a confidence interval for the population mean can be calculated as:

$$
\bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
$$

where:

- $$\bar{x}$$ = point estimate of the population mean (sample mean).
- $$z_{\alpha/2}$$ = reliability factor, a standard normal random variable for which the probability in the right-hand tail of the distribution is $$\alpha/2$$. In other words, this is the z-score that leaves $$\alpha/2$$ of probability in the upper tail.
- $$\frac{\sigma}{\sqrt{n}}$$ = the standard error of the sample mean where $$\sigma$$ is the known standard deviation of the population, and $$n$$ is the sample size.

The most commonly used standard normal distribution reliability factors are:

$$
z_{\alpha/2} = 1.645 \text{ for 90\% confidence intervals (the significance level is 10\%, 5\% in each tail).}
$$
$$
z_{\alpha/2} = 1.960 \text{ for 95\% confidence intervals (the significance level is 5\%, 2.5\% in each tail).}
$$
$$
z_{\alpha/2} = 2.575 \text{ for 99\% confidence intervals (the significance level is 1\%, 0.5\% in each tail).}
$$

Do these numbers look familiar? They should! In our review of common probability distributions, we found the probability under the standard normal curve between z = –1.96 and z = +1.96 to be 0.95, or 95\%. Owing to symmetry, this leaves a probability of 0.025 under each tail of the curve beyond z = –1.96 or z = +1.96, for a total of 0.05, or 5\%—just what we need for a significance level of 0.05, or 5\%.

:::info[**EXAMPLE: Confidence interval**]
Consider a practice exam that was administered to 36 Level I candidates. The mean score on this practice exam was 80. Assuming a population standard deviation equal to 15, construct and interpret a 99\% confidence interval for the mean score on the practice exam for 36 candidates. *Note that in this example the population standard deviation is known, so we don’t have to estimate it.*

**Answer:**
At a confidence level of 99\%, $$z_{\alpha/2} = z_{0.005} = 2.58$$. So, the 99\% confidence interval is calculated as follows:

$$
\bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}} = 80 \pm 2.58 \frac{15}{\sqrt{36}} = 80 \pm 6.45
$$

Thus, the 99\% confidence interval ranges from 73.55 to 86.45.
:::

:::info[**EXAMPLE: Confidence intervals for a population mean and for a single observation**]
Annual returns on energy stocks are approximately normally distributed with a mean of 9\% and standard deviation of 6\%. Construct a 90\% confidence interval for the annual returns of a randomly selected energy stock and a 90\% confidence interval for the mean of the annual returns for a sample of 12 energy stocks.

**Answer:**
A 90\% confidence interval for a single observation is 1.645 standard deviations from the sample mean.

$$
9\% \pm 1.645(6\%) = -0.87\% \text{ to } 18.87\%
$$

A 90\% confidence interval for the population mean is 1.645 standard errors from the sample mean.

$$
9\% \pm 1.645 \left( \frac{6\%}{\sqrt{12}} \right) = 6.15\% \text{ to } 11.85\%
$$
:::

Confidence intervals can be interpreted from a probabilistic perspective or a practical perspective. With regard to the outcome of the $\text{CFA}^\circledR$ practice exam example, these two perspectives can be described as follows:

- **Probabilistic interpretation.** After repeatedly taking samples of CFA candidates, administering the practice exam, and constructing confidence intervals for each sample’s mean, 99\% of the resulting confidence intervals will, in the long run, include the population mean.
- **Practical interpretation.** We are 99\% confident that the population mean score is between 73.55 and 86.45 for candidates from this population.

#### Confidence Intervals for the Population Mean: Normal With Unknown Variance

If the distribution of the population is normal with unknown variance, we can use the t-distribution to construct a confidence interval:

$$
\bar{x} \pm t_{\alpha/2} \frac{s}{\sqrt{n}}
$$

where:

- $$\bar{x}$$ = the point estimate of the population mean
- $$t_{\alpha/2}$$ = the reliability factor (a.k.a. t-statistic or critical t-value) corresponding to a t-distributed random variable with $$n - 1$$ degrees of freedom, where $$n$$ is the sample size. The area under the tail of the t-distribution to the right of $$t_{\alpha/2}$$ is $$\alpha/2$$
- $$\frac{s}{\sqrt{n}}$$ = standard error of the sample mean
- $$s$$ = sample standard deviation

Unlike the standard normal distribution, the reliability factors for the t-distribution depend on the sample size, so we can’t rely on a commonly used set of reliability factors. Instead, reliability factors for the t-distribution have to be looked up in a table of Student’s t-distribution, like the one at the back of this book.

Owing to the relatively fatter tails of the t-distribution, confidence intervals constructed using t-reliability factors $$(t_{\alpha/2})$$ will be more conservative (wider) than those constructed using z-reliability factors $$(z_{\alpha/2})$$.

:::info[**EXAMPLE: Confidence intervals**]
Let's return to the McCrary, Inc. example. Recall that we took a sample of the past 30 monthly stock returns for McCrary, Inc. and determined that the mean return was 2% and the sample standard deviation was 20%. Since the population variance is unknown, the standard error of the sample was estimated to be:
$$s_{\bar{x}} = \frac{s}{\sqrt{n}} = \frac{20\%}{\sqrt{30}} = 3.65\%$$

Now, let’s construct a 95% confidence interval for the mean monthly return.

**Answer**:
Here, we will use the reliability factor because the population variance is unknown. Since there are 30 observations, the degrees of freedom are $$n - 1 = 30 - 1 = 29$$. Remember, because this is a two-tailed test, at the 95% confidence level, the probability under each tail must be $$\alpha/2 = 2.5\%$$, for a total of 5%. So, referencing the one-sided probabilities for Student’s t-distribution at the back of this book, we find the critical t-value (reliability factor) at $$\alpha/2 = 0.025$$ and $$n - 1 = 29$$ to be $$t_{\alpha/2} = 2.045$$. Thus, the 95% confidence interval for the population mean is:
$$2\% \pm (t_{\alpha/2} \cdot s_{\bar{x}}) = 2\% \pm 2.045 (3.65\%) = 2\% \pm 7.4\%$$

Thus, the 95% confidence has a lower limit of -5.4% and an upper limit of 9.4%.

We can interpret this confidence interval by saying that we are 95% confident that the population mean monthly return for McCrary stock is between -5.4% and 9.4%.

:::note[**PROFESSOR'S NOTE**]
You should practice looking up reliability factors (i.e., critical t-values or t-statistics) in a t-table. The first step is always to compute the degrees of freedom, which is $$n - 1$$. The second step is to find the appropriate level of alpha or significance. This depends on whether the test you’re concerned with is one-tailed (like our example) or two-tailed (like our example).
:::